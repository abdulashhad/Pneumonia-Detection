{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nDATASET_DIR = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\nIMG_SIZE = 224  # MobileNetV3 input size\nBATCH_SIZE = 16\nEPOCHS = 10\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# -----------------------------\n# DATA PREPARATION\n# -----------------------------\ndef collect_image_paths(dataset_root):\n    import glob\n    train_path = os.path.join(dataset_root, \"train\")\n    test_path  = os.path.join(dataset_root, \"test\")\n\n    def make_df(path):\n        normal = glob.glob(os.path.join(path, \"NORMAL\", \"*\"))\n        pneumonia = glob.glob(os.path.join(path, \"PNEUMONIA\", \"*\"))\n        return pd.DataFrame({\n            \"image\": normal + pneumonia,\n            \"class\": [\"Normal\"] * len(normal) + [\"Pneumonia\"] * len(pneumonia)\n        })\n\n    return make_df(train_path), make_df(test_path)\n\ndef build_generator(df, img_size, batch_size, shuffle=True, augment=False):\n    datagen = ImageDataGenerator(\n        rescale=1./255,\n        zoom_range=0.1 if augment else 0.0,\n        width_shift_range=0.1 if augment else 0.0,\n        height_shift_range=0.1 if augment else 0.0,\n        horizontal_flip=augment\n    )\n    return datagen.flow_from_dataframe(\n        df, x_col=\"image\", y_col=\"class\",\n        target_size=(img_size, img_size),\n        class_mode=\"binary\",\n        batch_size=batch_size, shuffle=shuffle\n    )\n\n# -----------------------------\n# LOAD DATA\n# -----------------------------\ndf_train_all, df_test = collect_image_paths(DATASET_DIR)\ntrain_df, val_df = train_test_split(df_train_all, test_size=0.2,\n                                    stratify=df_train_all[\"class\"], random_state=SEED)\n\ntrain_gen = build_generator(train_df, IMG_SIZE, BATCH_SIZE, augment=True)\nval_gen   = build_generator(val_df, IMG_SIZE, BATCH_SIZE)\ntest_gen  = build_generator(df_test, IMG_SIZE, 1, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:38:04.294970Z","iopub.execute_input":"2025-10-13T14:38:04.295407Z","iopub.status.idle":"2025-10-13T14:38:08.571316Z","shell.execute_reply.started":"2025-10-13T14:38:04.295374Z","shell.execute_reply":"2025-10-13T14:38:08.570253Z"}},"outputs":[{"name":"stdout","text":"Found 4172 validated image filenames belonging to 2 classes.\nFound 1044 validated image filenames belonging to 2 classes.\nFound 624 validated image filenames belonging to 2 classes.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(\"Train:\", df_train_all['class'].value_counts())\nprint(\"Val:\", val_df['class'].value_counts())\nprint(\"Test:\", df_test['class'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:36:53.591381Z","iopub.status.idle":"2025-10-13T14:36:53.592407Z","shell.execute_reply.started":"2025-10-13T14:36:53.592172Z","shell.execute_reply":"2025-10-13T14:36:53.592199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(df_test))\nprint(\"Example train labels:\", train_df['class'].unique())\nprint(\"Class indices:\", train_gen.class_indices)\nprint(\"Validation accuracy last epoch:\", history.history['val_binary_accuracy'][-1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:39:34.439848Z","iopub.execute_input":"2025-10-13T14:39:34.440170Z","iopub.status.idle":"2025-10-13T14:39:34.447045Z","shell.execute_reply.started":"2025-10-13T14:39:34.440146Z","shell.execute_reply":"2025-10-13T14:39:34.445899Z"}},"outputs":[{"name":"stdout","text":"Train: 4172 Val: 1044 Test: 624\nExample train labels: ['Normal' 'Pneumonia']\nClass indices: {'Normal': 0, 'Pneumonia': 1}\nValidation accuracy last epoch: 0.7432950139045715\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# -----------------------------\n# MODEL (MobileNetV3 Small)\n# -----------------------------\nbase_model = tf.keras.applications.MobileNetV3Small(\n    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    include_top=False,\n    weights=\"imagenet\"\n)\n\nbase_model.trainable = False  # freeze base layers\n\ninputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nx = base_model(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.4)(x)\noutputs = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=keras.optimizers.Adam(1e-4),\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])\nmodel.summary()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:42:15.517367Z","iopub.execute_input":"2025-10-13T14:42:15.517667Z","iopub.status.idle":"2025-10-13T14:42:16.669930Z","shell.execute_reply.started":"2025-10-13T14:42:15.517648Z","shell.execute_reply":"2025-10-13T14:42:16.667033Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_10 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MobileNetV3Small (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │       \u001b[38;5;34m939,120\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m577\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MobileNetV3Small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">577</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m939,697\u001b[0m (3.58 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,697</span> (3.58 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m577\u001b[0m (2.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">577</span> (2.25 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m939,120\u001b[0m (3.58 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> (3.58 MB)\n</pre>\n"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# -----------------------------\n# MODEL (MobileNetV3 Small)\n# -----------------------------\nbase_model = tf.keras.applications.MobileNetV3Small(\n    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    include_top=False,\n    weights=None\n)\n\nbase_model.trainable = False  # freeze base layers\n\ninputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nx = base_model(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.3)(x)\noutputs = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=keras.optimizers.Adam(1e-4),\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])\nmodel.summary()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# TRAINING\n# -----------------------------\nes = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\nrlrop = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=2)\n\nhistory = model.fit(train_gen,\n                    validation_data=val_gen,\n                    epochs=EPOCHS,\n                    callbacks=[es, rlrop],\n                    verbose=1)\n\nmodel.save(\"pneumonia_mobilenetv3.h5\")\nprint(\"✅ Model saved as pneumonia_mobilenetv3.h5\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:22:07.029500Z","iopub.execute_input":"2025-10-13T13:22:07.029809Z","iopub.status.idle":"2025-10-13T13:22:07.040867Z","shell.execute_reply.started":"2025-10-13T13:22:07.029784Z","shell.execute_reply":"2025-10-13T13:22:07.039515Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2923623056.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mrlrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'callbacks' is not defined"],"ename":"NameError","evalue":"name 'callbacks' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# -----------------------------\n# EVALUATION\n# -----------------------------\npreds = model.predict(test_gen, verbose=1)\npred_labels = (preds > 0.5).astype(int).reshape(-1)\ny_true = df_test[\"class\"].map({\"Normal\":0, \"Pneumonia\":1}).values\n\nprint(\"Test Accuracy:\", accuracy_score(y_true, pred_labels))\nprint(classification_report(y_true, pred_labels, target_names=[\"Normal\",\"Pneumonia\"]))\nprint(\"ROC-AUC:\", roc_auc_score(y_true, preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:22:15.139422Z","iopub.execute_input":"2025-10-13T13:22:15.139754Z","iopub.status.idle":"2025-10-13T13:22:15.152624Z","shell.execute_reply.started":"2025-10-13T13:22:15.139730Z","shell.execute_reply":"2025-10-13T13:22:15.151023Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1650443302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# EVALUATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Normal\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Pneumonia\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"# -----------------------------\n# TFLITE INT8 QUANTIZATION\n# -----------------------------\nTFLITE_MODEL_PATH = \"pneumonia_mobilenetv3_int8.tflite\"\nN_REPRESENTATIVE = 100\n\ndef representative_dataset_gen_from_folder(img_folder, img_size, n=N_REPRESENTATIVE):\n    input_name = model.inputs[0].name\n    images = [os.path.join(img_folder, f) for f in os.listdir(img_folder) \n              if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n    images = random.sample(images, min(n, len(images)))\n    \n    for img_path in images:\n        img = cv2.imread(img_path)\n        if img is None:\n            continue\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (img_size, img_size))\n        img = np.expand_dims(img, axis=0).astype(np.uint8)  # INT8 input\n        yield {input_name: img}\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = lambda: representative_dataset_gen_from_folder(DATASET_DIR, IMG_SIZE)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.uint8\nconverter.inference_output_type = tf.uint8\n\ntflite_model = converter.convert()\n\nwith open(TFLITE_MODEL_PATH, \"wb\") as f:\n    f.write(tflite_model)\n\nprint(f\"✅ Fully INT8 quantized MobileNetV3 model saved as {TFLITE_MODEL_PATH}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nDATASET_DIR = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\nIMG_SIZE = 224\nBATCH_SIZE = 16\nSEED = 42\nEPOCHS_FEATURE_EXTRACTION = 5\nEPOCHS_FINE_TUNE = 15\n\n# -----------------------------\n# DATA PREPARATION\n# -----------------------------\ndef collect_image_paths(dataset_root):\n    import glob\n    train_path = os.path.join(dataset_root, \"train\")\n    test_path  = os.path.join(dataset_root, \"test\")\n\n    def make_df(path):\n        normal = glob.glob(os.path.join(path, \"NORMAL\", \"*\"))\n        pneumonia = glob.glob(os.path.join(path, \"PNEUMONIA\", \"*\"))\n        return pd.DataFrame({\n            \"image\": normal + pneumonia,\n            \"class\": [\"Normal\"] * len(normal) + [\"Pneumonia\"] * len(pneumonia)\n        })\n\n    return make_df(train_path), make_df(test_path)\n\ntrain_df_all, test_df = collect_image_paths(DATASET_DIR)\ntrain_df, val_df = train_test_split(train_df_all, test_size=0.2,\n                                    stratify=train_df_all[\"class\"], random_state=SEED)\n\n# Generators\ntrain_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n).flow_from_dataframe(\n    train_df, x_col=\"image\", y_col=\"class\",\n    target_size=(IMG_SIZE, IMG_SIZE),\n    class_mode=\"binary\",\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\nval_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n    val_df, x_col=\"image\", y_col=\"class\",\n    target_size=(IMG_SIZE, IMG_SIZE),\n    class_mode=\"binary\",\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\ntest_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n    test_df, x_col=\"image\", y_col=\"class\",\n    target_size=(IMG_SIZE, IMG_SIZE),\n    class_mode=\"binary\",\n    batch_size=1,\n    shuffle=False\n)\n\n# -----------------------------\n# MODEL (MobileNetV3Small)\n# -----------------------------\nbase_model = tf.keras.applications.MobileNetV3Small(\n    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    include_top=False,\n    weights=\"imagenet\"\n)\n\n# 1️⃣ Feature Extraction Stage\nbase_model.trainable = False\n\ninputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nx = base_model(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.4)(x)\noutputs = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=keras.optimizers.Adam(1e-4),\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])\n\n# -----------------------------\n# CALLBACKS\n# -----------------------------\nes = keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, restore_best_weights=True)\nrlrop = keras.callbacks.ReduceLROnPlateau(monitor='val_binary_accuracy', factor=0.3, patience=3, min_lr=1e-6)\n\n# -----------------------------\n# TRAINING - Feature Extraction\n# -----------------------------\nhistory_1 = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=EPOCHS_FEATURE_EXTRACTION,\n    callbacks=[es, rlrop],\n    verbose=1\n)\n\n# -----------------------------\n# FINE-TUNING STAGE\n# -----------------------------\nbase_model.trainable = True\n\n# Freeze lower layers (optional: keep first 100 layers frozen)\nfor layer in base_model.layers[:100]:\n    layer.trainable = False\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-5),\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)\n\nhistory_2 = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=EPOCHS_FINE_TUNE,\n    callbacks=[es, rlrop],\n    verbose=1\n)\n\n# -----------------------------\n# EVALUATION\n# -----------------------------\npreds = model.predict(test_gen, verbose=1)\npred_labels = (preds > 0.5).astype(int).reshape(-1)\ny_true = test_df[\"class\"].map({\"Normal\":0, \"Pneumonia\":1}).values\n\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n\nprint(\"Test Accuracy:\", accuracy_score(y_true, pred_labels))\nprint(classification_report(y_true, pred_labels, target_names=[\"Normal\",\"Pneumonia\"]))\nprint(\"ROC-AUC:\", roc_auc_score(y_true, preds))\n\n# -----------------------------\n# SAVE MODEL\n# -----------------------------\nmodel.save(\"pneumonia_mobilenetv3_finetuned.h5\")\nprint(\"✅ Model saved as pneumonia_mobilenetv3_finetuned.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:46:42.476376Z","iopub.execute_input":"2025-10-13T14:46:42.476763Z","iopub.status.idle":"2025-10-13T15:10:37.670847Z","shell.execute_reply.started":"2025-10-13T14:46:42.476737Z","shell.execute_reply":"2025-10-13T15:10:37.669539Z"}},"outputs":[{"name":"stdout","text":"Found 4172 validated image filenames belonging to 2 classes.\nFound 1044 validated image filenames belonging to 2 classes.\nFound 624 validated image filenames belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 472ms/step - binary_accuracy: 0.7315 - loss: 0.5923 - val_binary_accuracy: 0.7433 - val_loss: 0.5666 - learning_rate: 1.0000e-04\nEpoch 2/5\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 478ms/step - binary_accuracy: 0.7429 - loss: 0.5767 - val_binary_accuracy: 0.7433 - val_loss: 0.5649 - learning_rate: 1.0000e-04\nEpoch 3/5\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 467ms/step - binary_accuracy: 0.7517 - loss: 0.5632 - val_binary_accuracy: 0.7433 - val_loss: 0.5658 - learning_rate: 1.0000e-04\nEpoch 4/5\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 450ms/step - binary_accuracy: 0.7410 - loss: 0.5742 - val_binary_accuracy: 0.7433 - val_loss: 0.5628 - learning_rate: 1.0000e-04\nEpoch 5/5\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 456ms/step - binary_accuracy: 0.7440 - loss: 0.5694 - val_binary_accuracy: 0.7433 - val_loss: 0.5605 - learning_rate: 3.0000e-05\nEpoch 1/15\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 507ms/step - binary_accuracy: 0.7339 - loss: 0.5301 - val_binary_accuracy: 0.7433 - val_loss: 0.5675 - learning_rate: 1.0000e-05\nEpoch 2/15\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 491ms/step - binary_accuracy: 0.8649 - loss: 0.3057 - val_binary_accuracy: 0.7433 - val_loss: 0.5747 - learning_rate: 1.0000e-05\nEpoch 3/15\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 484ms/step - binary_accuracy: 0.8914 - loss: 0.2534 - val_binary_accuracy: 0.7433 - val_loss: 0.5848 - learning_rate: 1.0000e-05\nEpoch 4/15\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 485ms/step - binary_accuracy: 0.9061 - loss: 0.2268 - val_binary_accuracy: 0.7433 - val_loss: 0.5687 - learning_rate: 1.0000e-05\nEpoch 5/15\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 489ms/step - binary_accuracy: 0.9119 - loss: 0.2078 - val_binary_accuracy: 0.7433 - val_loss: 0.5687 - learning_rate: 3.0000e-06\nEpoch 6/15\n\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 488ms/step - binary_accuracy: 0.9168 - loss: 0.2043 - val_binary_accuracy: 0.7433 - val_loss: 0.5703 - learning_rate: 3.0000e-06\n\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step\nTest Accuracy: 0.625\n              precision    recall  f1-score   support\n\n      Normal       0.00      0.00      0.00       234\n   Pneumonia       0.62      1.00      0.77       390\n\n    accuracy                           0.62       624\n   macro avg       0.31      0.50      0.38       624\nweighted avg       0.39      0.62      0.48       624\n\nROC-AUC: 0.763866973482358\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved as pneumonia_mobilenetv3_finetuned.h5\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport random\n\n# -----------------------------\nIMG_SIZE = 224\nN_REPRESENTATIVE = 100\nTRAIN_IMG_FOLDER = \"/kaggle/input/chest-xray-pneumonia/chest_xray/train\"\nEXPORT_DIR = \"pneumonia_mobilenetv3_savedmodel\"\nTFLITE_MODEL_PATH = \"pneumonia_mobilenetv3_int8.tflite\"\n\n# -----------------------------\n# 1️⃣ EXPORT MODEL FOR TFLITE\n# -----------------------------\nmodel.export(EXPORT_DIR)\nprint(f\"✅ Model exported for TFLite at {EXPORT_DIR}\")\n\n# -----------------------------\n# 2️⃣ REPRESENTATIVE DATASET GENERATOR (float32)\n# -----------------------------\ndef representative_dataset_gen(img_folder, img_size, n=N_REPRESENTATIVE):\n    images = []\n    for root, dirs, files in os.walk(img_folder):\n        for file in files:\n            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                images.append(os.path.join(root, file))\n    images = random.sample(images, min(n, len(images)))\n\n    for img_path in images:\n        img = cv2.imread(img_path)\n        if img is None:\n            continue\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (img_size, img_size))\n        img = img.astype(np.float32) / 255.0   # <-- IMPORTANT: normalize to [0,1]\n        img = np.expand_dims(img, axis=0)\n        yield [img]\n\n# -----------------------------\n# 3️⃣ CONVERT TO FULLY INT8 TFLITE\n# -----------------------------\nconverter = tf.lite.TFLiteConverter.from_saved_model(EXPORT_DIR)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = lambda: representative_dataset_gen(TRAIN_IMG_FOLDER, IMG_SIZE)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.uint8   # input will be uint8 on device\nconverter.inference_output_type = tf.uint8  # output will be uint8 on device\n\ntflite_model = converter.convert()\n\n# -----------------------------\n# 4️⃣ SAVE TFLITE MODEL\n# -----------------------------\nwith open(TFLITE_MODEL_PATH, \"wb\") as f:\n    f.write(tflite_model)\n\nprint(f\"✅ Fully INT8 quantized TFLite model saved as {TFLITE_MODEL_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:25:30.614880Z","iopub.execute_input":"2025-10-13T15:25:30.615780Z","iopub.status.idle":"2025-10-13T15:26:11.078926Z","shell.execute_reply.started":"2025-10-13T15:25:30.615749Z","shell.execute_reply":"2025-10-13T15:26:11.078099Z"}},"outputs":[{"name":"stdout","text":"Saved artifact at 'pneumonia_mobilenetv3_savedmodel'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_1250')\nOutput Type:\n  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\nCaptures:\n  134829961982800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829961983568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829961984528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829961984144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829961983952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829961985872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829961980496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924041360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829961985488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829961984720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924042512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924043088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924042128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924043856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924043280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924044432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924044816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924044624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924040784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924045968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924046352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924046736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924046544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924040976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924047888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924048272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924048656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924048464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924044048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924049808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924050192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924050576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924050384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924045584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924051728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924052112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924052496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924052304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924047504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924053648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924054032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924054416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924054224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924049424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924055568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924055952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924056720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924056144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924051344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924054800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913571408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913572560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829924055184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913572368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913573712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913574096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913574480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913574288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913572176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913575632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913576208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913575248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913576976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913576400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913577552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913577936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913577744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913573328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913579088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913579472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913579856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913579664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913571792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913581008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913581392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913581776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913581584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913577168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913582928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913583504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913582544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913584272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913583696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913584848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913585232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913585040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913580624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913586384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913586768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913587536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913586960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913571600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913578704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914129424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914128848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913584464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914129808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914130960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914131536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914130576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914132304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914131728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914132880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914133264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914133072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914128464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914134416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914134800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914135184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914134992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914129616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914136336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914136720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914137104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914136912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914132496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914138256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914138832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914137872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914139600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914139024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914140176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914140560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914140368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914135952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914141712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914142096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914142480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914142288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914129040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914143632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914143248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914141328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914144208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914139792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829914142864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912655248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912654672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912656016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912653904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912656592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912656976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912656784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912655440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912658128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912658512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912658896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912658704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912654288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912660048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912660432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912660816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912660624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912656208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912661968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912662544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912661584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912663312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912662736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912663888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912664272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912664080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912659664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912665424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912665808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912666192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912666000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912654096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912667344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912667728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912668112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912667920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912663504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912669264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912669840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912668880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912670032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912657744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913310224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913309264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829912669648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913309456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913311760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913312144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913312528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913312336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913309648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913313680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913314064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913314448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913314256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913310608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913315600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913316176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913315216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913316944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913316368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913317520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913317904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913317712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913313296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913319056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913319440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913319824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913319632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913310032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913320976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913321552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913318672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134829913322704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n✅ Model exported for TFLite at pneumonia_mobilenetv3_savedmodel\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1760369159.532597      37 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\nW0000 00:00:1760369159.532637      37 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n","output_type":"stream"},{"name":"stdout","text":"✅ Fully INT8 quantized TFLite model saved as pneumonia_mobilenetv3_int8.tflite\n","output_type":"stream"},{"name":"stderr","text":"fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n","output_type":"stream"}],"execution_count":31}]}